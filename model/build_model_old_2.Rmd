---
title: "Build Prediction Model"
output:
  html_document:
    df_print: paged
---

```{r}
sampleSize <- 40/100 # sample size in percents
set.seed(1234)
```


```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tm)
library(stringr)
library(dplyr)
library(wordcloud)
library(tidytext)
```

```{r readData, warning=FALSE, cache=TRUE}

basicDir <- "../data/final/en_US/"
twitterFilePath <- paste(basicDir, "en_US.twitter.txt", sep = "")
blogsFilePath <- paste(basicDir, "en_US.blogs.txt", sep = "")
newsFilePath <- paste(basicDir, "en_US.news.txt", sep = "")
twitterData <- readLines(twitterFilePath, encoding="UTF-8", skipNul = TRUE)
blogsData <- readLines(blogsFilePath, encoding="UTF-8", skipNul = TRUE)
newsData <- readLines(newsFilePath, encoding="UTF-8", skipNul = TRUE)

```

```{r data, cache=TRUE}
data <- c(twitterData, blogsData, newsData)

indices <- function(){sample(seq_len(length(data)), length(data)*sampleSize)}
indices <- indices()
data <- data[indices]

rm(twitterData)
rm(newsData)
rm(blogsData)
gc()

```

```{r cleanData}

# make lowercase
data <- tolower(data)

# replace the non-alphanumeric characters with spaces
# but only when at the beginning/end of the word
data <- gsub("\\s['.,:;!?(){}<>]+(\\w)", " \\1", data)
data <- gsub("(\\w)['.,:;!?(){}<>]+\\s", "\\1 ", data)

# remove all words made of pure numbers and special characters
data <- gsub("\\s+[0-9'.,:;!?(){}<>]+\\s+", " ", data)
data <- gsub("^[0-9'.,:;!?(){}<>]+\\s+", " ", data)
data <- gsub("\\s+[0-9'.,:;!?(){}<>]+$", " ", data)

# replace spaces at the beginning of the line
data <- gsub("^\\s+", "", data)

# replace spaces at the end of the line
data <- gsub("\\s+$", "", data)

# replace all multiple spaces to one
data <- gsub("\\s+", " ", data)

df <- data.frame(data)
colnames(df) <- c("text")

write.csv(df, "converted.data.csv", row.names = FALSE)

rm(data)
gc()
```

```{r}
format.number <- function(n) {
  format(n, big.mark = ",")
}
```


```{r}

splitText <- function(ngram) {
  parts <- unlist(strsplit(ngram, " "))
  size <- length(parts)
  list(
    paste(parts[1:size - 1], collapse = " "),
    parts[size]
  )
}

getText <- function(ngram) {
  splitText(ngram)[[1]]
}

getNextTerm <- function(ngram) {
  splitText(ngram)[[2]]
}


buildNGrams <- function(data, n, fileName) {
  
  colNames <- sapply(1:(n - 1), function(x){paste("word", x, sep = "")})
  
  result <- data %>%
    unnest_tokens(ngram, text, token = "ngrams", n = n) %>%
    dplyr::count(ngram, sort = TRUE) %>%
    separate(ngram, c(colNames, "nextTerm")) %>%
    unite("phrase", colNames, sep = " ")
  write.csv(result, fileName, row.names = FALSE)
  rm(result)
}
```


```{r monograms, cache=TRUE}
#buildNGrams(corpusDF, 1, "monograms.csv")
gc()
```

```{r bigrams, cache=FALSE}
buildNGrams(df, 2, "bigrams.csv")
gc()
```

```{r trigrams, cache=FALSE}
buildNGrams(df, 3, "trigrams.csv")
gc()
```

```{r quadgrams, cache=FALSE}
buildNGrams(df, 4, "quadgrams.csv")
#gc()
```


